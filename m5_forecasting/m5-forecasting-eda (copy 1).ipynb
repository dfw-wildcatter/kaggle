{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EXPLORATORY DATA ANALYSIS FOR M5**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **INITIALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load required packages\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('bmh')\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "import gc\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ignore warnings from sklearn and seaborn\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn\n",
    "\n",
    "# pandas output format\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calendar.csv\n",
      "M5-Competitors-Guide-Final-10-March-2020.odt\n",
      "m5-forecasting-eda.ipynb\n",
      "sales_train_validation.csv\n",
      "sample_submission.csv\n",
      "sell_prices.csv\n",
      "SGB-m5-forecasting.ipynb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check files available\n",
    "from subprocess import check_output\n",
    "print(check_output(['ls', os.getcwd()]).decode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **EXPLORATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cal_dtypes = {'event_name_1': 'category', 'event_name_2': 'category', \n",
    "              'event_type_1': 'category', 'event_type_2': 'category',\n",
    "              'weekday': 'category', 'wm_yr_wk': 'int16', 'wday': 'int16',\n",
    "              'month': 'int16', 'year': 'int16', 'snap_CA': 'float32', \n",
    "              'snap_TX': 'float32', 'snap_WI': 'float32'}\n",
    "price_dtypes = {'store_id': 'category', 'item_id': 'category', 'wm_yr_wk': 'int16',\n",
    "               'sell_price': 'float32'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 4, 25, 0, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters for constructing time series\n",
    "h = 28 # forecast horizon\n",
    "max_lags = 57\n",
    "tr_last = 1913 # last training observation\n",
    "fday = datetime(2016, 4, 25) # forecast start date\n",
    "fday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct time series\n",
    "def create_df(is_train = True, nrows = None, first_day = 1200):\n",
    "    prices = pd.read_csv('sell_prices.csv', dtype = price_dtypes)\n",
    "    for col, col_dtype in price_dtypes.items():\n",
    "        if col_dtype == 'category':\n",
    "            prices[col] = prices[col].cat.codes.astype('int16')\n",
    "            prices[col] -= prices[col].min() # scaling\n",
    "    cal = pd.read_csv('calendar.csv', dtype = cal_dtypes)\n",
    "    cal['date'] = pd.to_datetime(cal['date'])\n",
    "    for col, col_dtype in cal_dtypes.items():\n",
    "        if col_dtype == 'category':\n",
    "            cal[col] = cal[col].cat.codes.astype('int16')\n",
    "            cal[col] -= cal[col].min()\n",
    "    \n",
    "    start_day = max(1 if is_train else tr_last - max_lags, first_day)\n",
    "    numcols = [f'd_{day}' for day in range(start_day, tr_last+1)] #sales data rolling window\n",
    "    catcols = ['id', 'item_id', 'dept_id', 'store_id', 'cat_id', 'state_id']\n",
    "    dtype = {numcol: 'float32' for numcol in numcols}\n",
    "    dtype.update({col: 'category' for col in catcols if col != 'id'})\n",
    "    df = pd.read_csv('sales_train_validation.csv', nrows = nrows, \n",
    "                     usecols = catcols + numcols, dtype = dtype)\n",
    "    for col in catcols:\n",
    "        if col != 'id':\n",
    "            df[col] = df[col].cat.codes.astype('int16')\n",
    "            df[col] -= df[col].min()\n",
    "    if not is_train:\n",
    "        for day in range(tr_last + 1, tr_last + 28 + 1):\n",
    "            df[f'd_{day}'] = np.nan\n",
    "    df = pd.melt(df, \n",
    "                 id_vars = catcols,\n",
    "                 value_vars = [col for col in df.columns if col.startswith('d_')], # numeric\n",
    "                 var_name = 'd', # day\n",
    "                 value_name = 'sales')\n",
    "    df = df.merge(cal, on='d', copy = False)\n",
    "    df = df.merge(prices, on = ['store_id', 'item_id', 'wm_yr_wk'], copy=False)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create forecast series\n",
    "def create_fea(df):\n",
    "    lags = [7, 28]\n",
    "    lag_cols = [f'lag_{lag}' for lag in lags]\n",
    "    for lag, lag_col in zip(lags, lag_cols):\n",
    "        df[lag_col] = df[['id', 'sales']].groupby('id')['sales'].shift(lag)\n",
    "        \n",
    "    wins = [7, 28] # windows\n",
    "    for win in wins:\n",
    "        for lag, lag_col in zip(lags, lag_cols):\n",
    "            df[f'rmean_{lag}_{win}'] = df[['id', lag_col]].groupby('id')[lag_col].transform(lambda x: x.rolling(win).mean())\n",
    "    \n",
    "    date_features = {\n",
    "        'wday': 'weekday',\n",
    "        'week': 'weekofyear',\n",
    "        'month': 'month',\n",
    "        'quarter': 'quarter',\n",
    "        'year': 'year',\n",
    "        'mday': 'day'}\n",
    "    \n",
    "    for date_feat_name, date_feat_func in date_features.items():\n",
    "        if date_feat_name in df.columns:\n",
    "            df[date_feat_name] = df[date_feat_name].astype('int16')\n",
    "        else:\n",
    "            df[date_feat_name] = getattr(df['date'].dt, date_feat_func).astype('int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (40718219, 1) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-c92bc114a234>\u001b[0m in \u001b[0;36mcreate_df\u001b[0;34m(is_train, nrows, first_day)\u001b[0m\n\u001b[1;32m     33\u001b[0m                  value_name = 'sales')\n\u001b[1;32m     34\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'store_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wm_yr_wk'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   7295\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7296\u001b[0m             \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7297\u001b[0;31m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7298\u001b[0m         )\n\u001b[1;32m   7299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     )\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mllabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m         )\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   2020\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2021\u001b[0m             b = make_block(\n\u001b[0;32m-> 2022\u001b[0;31m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2023\u001b[0m                 \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2024\u001b[0m             )\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m    246\u001b[0m     to_concat = [\n\u001b[1;32m    247\u001b[0m         \u001b[0mju\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     ]\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    246\u001b[0m     to_concat = [\n\u001b[1;32m    247\u001b[0m         \u001b[0mju\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     ]\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/pandas/core/internals/concat.py\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[0;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.6/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"F\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     func = _get_take_nd_function(\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (40718219, 1) and data type float32"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = create_df(is_train=True, first_day = 50500) #skip days to save on memory\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load sale date and sale price features\n",
    "sale_date = pd.read_csv('calendar.csv') # date of sales\n",
    "sale_price = pd.read_csv('sell_prices.csv') # price of items sold\n",
    "labels = sale_date['wm_yr_wk'].values\n",
    "#print(calendar.info())\n",
    "print(sale_date.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean up sale price table\n",
    "sale_price['id'] =  sale_price['item_id'] + '_' + sale_price['store_id'] + '_validation'\n",
    "sale_price.drop(['store_id', 'item_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean up df, group by id and transpose\n",
    "df2 = df.copy()\n",
    "ls = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "df2.drop(ls, axis=1, inplace=True)\n",
    "df2 = df2.set_index('id').T\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test\n",
    "tmp = pd.DataFrame(df2.iloc[:, 1])\n",
    "tmp['wm_yr_wk'] = labels[:tmp.shape[0]]\n",
    "tmp2 = sale_price[sale_price['id'] == 'HOBBIES_1_002_CA_1_validation']\n",
    "new_df = pd.merge(tmp, tmp2, on = 'wm_yr_wk', how = 'left')\n",
    "print(new_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "sales = dict()\n",
    "for j in range(1, df2.shape[1]):\n",
    "    sales_id = df2.columns[j]\n",
    "    tmp = pd.DataFrame(df2.iloc[:, j])\n",
    "    tmp['wm_yr_wk'] = labels[:tmp.shape[0]]\n",
    "    tmp2 = sale_price[sale_price['id'] == sales_id]\n",
    "    tmp2 = pd.merge(tmp, tmp2, on = 'wm_yr_wk', how = 'left')\n",
    "    sales[sales_id] = tmp2.drop('id', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start off with 1 item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split day columns into 20 chunks for train and test\n",
    "#print(df.columns[-1])\n",
    "chunks = dict()\n",
    "for chunk_id in range(20):\n",
    "    col_start = 90 * chunk_id + 1\n",
    "    chunks[chunk_id] = df.iloc[:, col_start:col_start + 90]\n",
    "chunks[20] = df.iloc[:, 1801:1914]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define function to split each chunk into train and test\n",
    "def split_train_test(chunks):\n",
    "    train, test = list(), list()\n",
    "    # first 60 days of observations for train\n",
    "    cut_point = 60\n",
    "    # enumerate chunks\n",
    "    for rows, cols in chunks.items():\n",
    "        # split chunk columns by 'position_within_chunk'\n",
    "        train_cols = cols.iloc[:, :cut_point]\n",
    "        test_cols = cols.iloc[:, cut_point:]\n",
    "        if len(train_cols) == 0 or len(test_cols) == 0:\n",
    "            continue\n",
    "        train.append(train_cols)\n",
    "        test.append(test_cols)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return a list of relative forecast lead times\n",
    "# in reality need to forecast day+1, day+2, ... day+28\n",
    "#def get_lead_times():\n",
    "#    return [1, 7, 14, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "train, test = split_train_test(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the test data set in chunks to [chunk][variable][time] format\n",
    "def prepare_test_forecasts(test_chunks):\n",
    "    predictions = list()\n",
    "    # enumerate chunks to forecast\n",
    "    for cols in test_chunks:\n",
    "        # enumerate targets for chunk\n",
    "        chunk_predictions = list()\n",
    "        for j in range(cols.shape[0]):\n",
    "            yhat = cols[j, :]\n",
    "            chunk_predictions.append(yhat)\n",
    "        chunk_predictions = np.array(chunk_predictions)\n",
    "        predictions.append(chunk_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# distribution plot of target variable\n",
    "y = train['SalePrice']\n",
    "print(y.describe())\n",
    "\n",
    "sns.distplot(y, fit=norm);\n",
    "(mu, sigma) = norm.fit(y)\n",
    "print('\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f})'.format(mu, sigma)], loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "# QQ plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(y, plot=plt)\n",
    "plt.show()\n",
    "\n",
    "# therefore, logscale distribution is better\n",
    "#plt.figure(figsize=(10,10))\n",
    "#plt.hist(y, bins=np.geomspace(y.min(), y.max(), 50))\n",
    "#plt.xscale('log')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FEATURE ENGINEERING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# apply log transform to target variable\n",
    "train['SalePrice'] = np.log1p(train['SalePrice'])\n",
    "sns.distplot(train['SalePrice'], fit=norm); # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "n_train = train.shape[0]; n_test = test.shape[0]\n",
    "y = train['SalePrice'].values\n",
    "df = pd.concat((train, test)).reset_index(drop=True)\n",
    "del df['SalePrice']\n",
    "print(n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# deal with missing data\n",
    "df_nan = df.isnull().sum() / len(df) * 100\n",
    "df_nan = df_nan.drop(df_nan[df_nan == 0].index).sort_values(ascending=False)\n",
    "print(df_nan[:10])\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10,10))\n",
    "plt.xticks(rotation='90')\n",
    "sns.barplot(x=df_nan.index[:10], y=df_nan[:10])\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('% missing', fontsize=12)\n",
    "plt.title('% missing by feature', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x-correlation map\n",
    "corrmat = train.corr()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corrmat, vmax=0.9)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# deal with missing and error values\n",
    "df2 = df.copy()\n",
    "df2.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "# all below from https://www.kaggle.com/juliensiems/cleaning-new-features-gps-coordinates-included\n",
    "\n",
    "df2['LotFrontage'] = df2.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# replace missing values with zeros\n",
    "ls = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtHalfBath', 'BsmtFullBath', 'BsmtUnfSF', 'TotalBsmtSF',\n",
    "      'EnclosedPorch', 'Fireplaces', 'GarageArea', 'GarageCars', 'GarageYrBlt', 'KitchenAbvGr', \n",
    "      'MasVnrArea', 'MiscVal', 'OpenPorchSF', 'PoolArea','ScreenPorch', 'TotRmsAbvGrd', 'WoodDeckSF']\n",
    "for f in ls:\n",
    "    df2[f].fillna(0, inplace=True)\n",
    "    \n",
    "# relace missing values with labels\n",
    "ls_no = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Fence',\n",
    "        'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'MiscFeature','PoolQC']\n",
    "for f in ls_no:\n",
    "    df2[f].fillna(\"No\", inplace=True)\n",
    "    \n",
    "# replace missing values with other labels\n",
    "ls_ta = ['ExterCond', 'ExterQual', 'HeatingQC', 'KitchenQual']\n",
    "ls_norm = ['Condition1', 'Condition2']\n",
    "for f in ls_ta:\n",
    "    df2[f].fillna(\"TA\", inplace=True)\n",
    "for f in ls_norm:\n",
    "    df2[f].fillna(\"Norm\", inplace=True)\n",
    "\n",
    "df2['Alley'].fillna('None', inplace=True)\n",
    "df2['CentralAir'].fillna('N', inplace=True)\n",
    "df2['PavedDrive'].fillna('N', inplace=True)\n",
    "df2['MasVnrType'].fillna('None', inplace=True)\n",
    "\n",
    "ls = ['MSZoning', 'Utilities', 'Electrical', 'SaleCondition', 'SaleType', 'LotShape', 'Functional',\n",
    "      'Exterior2nd', 'Exterior1st']\n",
    "for f in ls:\n",
    "    df2[f].fillna(df2[f].mode()[0], inplace=True)\n",
    "\n",
    "# add features to replace neighborhood by its coordinates\n",
    "\n",
    "df2['lat'] = df2['Neighborhood'].values\n",
    "df2['lon'] = df2['Neighborhood'].values\n",
    "\n",
    "df2['lat'].replace({'Blmngtn' : 42.062806,\n",
    "                                               'Blueste' : 42.009408,\n",
    "                                                'BrDale' : 42.052500,\n",
    "                                                'BrkSide': 42.033590,\n",
    "                                                'ClearCr': 42.025425,\n",
    "                                                'CollgCr': 42.021051,\n",
    "                                                'Crawfor': 42.025949,\n",
    "                                                'Edwards': 42.022800,\n",
    "                                                'Gilbert': 42.027885,\n",
    "                                                'GrnHill': 42.000854,\n",
    "                                                'IDOTRR' : 42.019208,\n",
    "                                                'Landmrk': 42.044777,\n",
    "                                                'MeadowV': 41.991866,\n",
    "                                                'Mitchel': 42.031307,\n",
    "                                                'NAmes'  : 42.042966,\n",
    "                                                'NoRidge': 42.050307,\n",
    "                                                'NPkVill': 42.050207,\n",
    "                                                'NridgHt': 42.060356,\n",
    "                                                'NWAmes' : 42.051321,\n",
    "                                                'OldTown': 42.028863,\n",
    "                                                'SWISU'  : 42.017578,\n",
    "                                                'Sawyer' : 42.033611,\n",
    "                                                'SawyerW': 42.035540,\n",
    "                                                'Somerst': 42.052191,\n",
    "                                                'StoneBr': 42.060752,\n",
    "                                                'Timber' : 41.998132,\n",
    "                                                'Veenker': 42.040106}, inplace=True)\n",
    "\n",
    "df2['lon'].replace({'Blmngtn' : -93.639963,\n",
    "                                               'Blueste' : -93.645543,\n",
    "                                                'BrDale' : -93.628821,\n",
    "                                                'BrkSide': -93.627552,\n",
    "                                                'ClearCr': -93.675741,\n",
    "                                                'CollgCr': -93.685643,\n",
    "                                                'Crawfor': -93.620215,\n",
    "                                                'Edwards': -93.663040,\n",
    "                                                'Gilbert': -93.615692,\n",
    "                                                'GrnHill': -93.643377,\n",
    "                                                'IDOTRR' : -93.623401,\n",
    "                                                'Landmrk': -93.646239,\n",
    "                                                'MeadowV': -93.602441,\n",
    "                                                'Mitchel': -93.626967,\n",
    "                                                'NAmes'  : -93.613556,\n",
    "                                                'NoRidge': -93.656045,\n",
    "                                                'NPkVill': -93.625827,\n",
    "                                                'NridgHt': -93.657107,\n",
    "                                                'NWAmes' : -93.633798,\n",
    "                                                'OldTown': -93.615497,\n",
    "                                                'SWISU'  : -93.651283,\n",
    "                                                'Sawyer' : -93.669348,\n",
    "                                                'SawyerW': -93.685131,\n",
    "                                                'Somerst': -93.643479,\n",
    "                                                'StoneBr': -93.628955,\n",
    "                                                'Timber' : -93.648335,\n",
    "                                                'Veenker': -93.657032}, inplace=True)\n",
    "\n",
    "# create new features by combining existing features\n",
    "df2['IsRegularLotShape'] = (df2['LotShape'] =='Reg') * 1\n",
    "df2['IsLandLevel'] = (df2['LandContour'] == 'Lvl') * 1\n",
    "df2['IsLandSlopeGentle'] = (df2['LandSlope'] == 'Gtl') * 1\n",
    "df2['IsElectricalSBrkr'] = (df2['Electrical'] == 'SBrkr') * 1\n",
    "df2['IsGarageDetached'] = (df2['GarageType'] == 'Detchd') * 1\n",
    "df2['IsPavedDrive'] = (df2['PavedDrive'] == 'Y') * 1\n",
    "df2['HasShed'] = (df2['MiscFeature'] == 'Shed') * 1.\n",
    "df2['Remodeled'] = (df2['YearRemodAdd'] != df2['YearBuilt']) * 1\n",
    "df2['RecentRemodel'] = (df2['YearRemodAdd'] == df2['YrSold']) * 1\n",
    "df2['VeryNewHouse'] = (df2['YearBuilt'] == df2['YrSold']) * 1\n",
    "df2['HasMasVnr'] = (df2['MasVnrArea'] == 0) * 1\n",
    "df2['HasWoodDeck'] = (df2['WoodDeckSF'] == 0) * 1\n",
    "df2['HasOpenPorch'] = (df2['OpenPorchSF'] == 0) * 1\n",
    "df2['HasEnclosedPorch'] = (df2['EnclosedPorch'] == 0) * 1\n",
    "df2['Has3SsnPorch'] = (df2['3SsnPorch'] == 0) * 1\n",
    "df2['HasScreenPorch'] = (df2['ScreenPorch'] == 0) * 1\n",
    "\n",
    "# encode categorical variables\n",
    "df2 = df2.replace({'Alley' : {'Grvl' : 1, 'Pave' : 2},\n",
    "                           'BsmtCond' : {'No' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n",
    "                           'BsmtExposure' : {'No' : 0, 'Mn' : 1, 'Av': 2, 'Gd' : 3},\n",
    "                           'BsmtFinType1' : {'No' : 0, 'Unf' : 1, 'LwQ': 2, 'Rec' : 3, 'BLQ' : 4,\n",
    "                                             'ALQ' : 5, 'GLQ' : 6},\n",
    "                           'BsmtFinType2' : {'No' : 0, 'Unf' : 1, 'LwQ': 2, 'Rec' : 3, 'BLQ' : 4,\n",
    "                                             'ALQ' : 5, 'GLQ' : 6},\n",
    "                           'BsmtQual' : {'No' : 0, 'Po' : 1, 'Fa' : 2, 'TA': 3, 'Gd' : 4, 'Ex' : 5},\n",
    "                           'ExterCond' : {'Po' : 1, 'Fa' : 2, 'TA': 3, 'Gd': 4, 'Ex' : 5},\n",
    "                           'ExterQual' : {'Po' : 1, 'Fa' : 2, 'TA': 3, 'Gd': 4, 'Ex' : 5},\n",
    "                           'FireplaceQu' : {'No' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n",
    "                           'Functional' : {'Sal' : 1, 'Sev' : 2, 'Maj2' : 3, 'Maj1' : 4, 'Mod': 5,\n",
    "                                           'Min2' : 6, 'Min1' : 7, 'Typ' : 8},\n",
    "                           'GarageCond' : {'No' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n",
    "                           'GarageQual' : {'No' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n",
    "                           'HeatingQC' : {'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n",
    "                           'KitchenQual' : {'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n",
    "                           'LandSlope' : {'Sev' : 1, 'Mod' : 2, 'Gtl' : 3},\n",
    "                           'LotShape' : {'IR3' : 1, 'IR2' : 2, 'IR1' : 3, 'Reg' : 4},\n",
    "                           'PavedDrive' : {'N' : 0, 'P' : 1, 'Y' : 2},\n",
    "                           'PoolQC' : {'No' : 0, 'Fa' : 1, 'TA' : 2, 'Gd' : 3, 'Ex' : 4},\n",
    "                           'Street' : {'Grvl' : 1, 'Pave' : 2},\n",
    "                           'Utilities' : {'ELO' : 1, 'NoSeWa' : 2, 'NoSewr' : 3, 'AllPub' : 4}})\n",
    "\n",
    "# combining existing features\n",
    "df2['OverallGrade'] = df2['OverallQual'] * df2['OverallCond']\n",
    "df2['GarageGrade'] = df2['GarageQual'] * df2['GarageCond']\n",
    "df2['ExterGrade'] = df2['ExterQual'] * df2['ExterCond']\n",
    "df2['KitchenScore'] = df2['KitchenAbvGr'] * df2['KitchenQual']\n",
    "df2['FireplaceScore'] = df2['Fireplaces'] * df2['FireplaceQu']\n",
    "df2['GarageScore'] = df2['GarageArea'] * df2['GarageQual']\n",
    "df2['PoolScore'] = df2['PoolArea'] * df2['PoolQC']\n",
    "df2['TotalBath'] = df2['BsmtFullBath'] + (0.5 * df2['BsmtHalfBath']) + df2['FullBath'] + (0.5 * df2['HalfBath'])\n",
    "df2['AllSF'] = df2['GrLivArea'] + df2['TotalBsmtSF']\n",
    "df2['AllFlrsSF'] = df2['1stFlrSF'] + df2['2ndFlrSF']\n",
    "df2['AllPorchSF'] = df2['OpenPorchSF'] + df2['EnclosedPorch'] + df2['3SsnPorch'] + df2['ScreenPorch']\n",
    "df2['HasMasVnr'] = df2.MasVnrType.replace({'BrkCmn' : 1, 'BrkFace' : 1, 'CBlock' : 1,\n",
    "                                                   'Stone' : 1, 'None' : 0})\n",
    "df2['SaleCondition_PriceDown'] = df2.SaleCondition.replace({'Abnorml': 1,\n",
    "                                                              'Alloca': 1,\n",
    "                                                              'AdjLand': 1,\n",
    "                                                              'Family': 1,\n",
    "                                                              'Normal': 0,\n",
    "                                                              'Partial': 0})\n",
    "\n",
    "df2['BoughtOffPlan'] = df2.SaleCondition.replace({'Abnorml' : 0, 'Alloca' : 0, 'AdjLand' : 0,\n",
    "                                                          'Family' : 0, 'Normal' : 0, 'Partial' : 1})\n",
    "\n",
    "# taken from https://www.kaggle.com/yadavsarthak/house-prices-advanced-regression-techniques/you-got-this-feature-engineering-and-lasso\n",
    "df2['1stFlr_2ndFlr_Sf'] = np.log1p(df2['1stFlrSF'] + df2['2ndFlrSF'])\n",
    "df2['All_Liv_SF'] = np.log1p(df2['1stFlr_2ndFlr_Sf'] + df2['LowQualFinSF'] + df2['GrLivArea'])\n",
    "\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check and replace any remaining missing values\n",
    "df2_nan = df2.isnull().sum() / len(df2) * 100\n",
    "df2_nan = df2_nan.drop(df2_nan[df2_nan == 0].index).sort_values(ascending=False)\n",
    "print(df2_nan[0:5])\n",
    "\n",
    "#for f in ls:\n",
    "#    df2[f] = df2[f].apply(lambda x: x.fillna(x.median(), axis=0) # for numerical features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform some numerical variables to categorical\n",
    "ls =['MSSubClass', 'YrSold', 'MoSold']\n",
    "for f in ls:\n",
    "    df2[f] = df2[f].astype(str)\n",
    "    \n",
    "# label encoding for categorical variables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for f in ls:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(df2[f].values))\n",
    "    df2[f] = lbl.transform(list(df2[f].values))\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# split between numerical and categorical features\n",
    "df_num = df2.select_dtypes(include = ['float64', 'int64']) # 109 features + SalePrice\n",
    "num_skewed = df_num.apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'Skew': num_skewed})\n",
    "print(skewness.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# box-cox transformation of highly skewed features\n",
    "skewness = skewness[abs(skewness) > 0.75]\n",
    "skewness.drop('lat', inplace=True)\n",
    "skewness.drop('lon', inplace=True)\n",
    "print(skewness.shape[0])\n",
    "lam=0.15\n",
    "from scipy.special import boxcox1p\n",
    "for f in skewness.index:\n",
    "    if (f != 'lon') | (str(f)!= 'lat'):\n",
    "            print(f)\n",
    "            df2[f] = boxcox1p(df2[f], lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dummies for categorical variables\n",
    "df3 = df2.copy() #keep original df\n",
    "df3 = pd.get_dummies(df3)\n",
    "print(df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split between train and test after feature engineering\n",
    "train = df3[:n_train]; train['Id'] = train_id.values; train.set_index('Id')\n",
    "test = df3[n_train:]; test['Id'] = test_id.values; test.set_index('Id')\n",
    "outcomes = pd.DataFrame({'SalePrice': y})\n",
    "outcomes['Id'] = train_id.values; outcomes.set_index('Id')\n",
    "\n",
    "train.to_csv('train_engineered.csv')\n",
    "test.to_csv('test_engineered.csv')\n",
    "outcomes.to_csv('outcomes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FEATURES SHORTLIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make shortlist of features highly correlated to target variable\n",
    "df4 = train.copy(); df4['SalePrice'] = y\n",
    "df_xycorrs = df4.corr().iloc[:-1,-1]\n",
    "features_rank = df_xycorrs[abs(df_xycorrs) > 0.3].sort_values(ascending=False)\n",
    "features_shortlist = features_rank.index.tolist()\n",
    "print(features_shortlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot correlations between numerical features and target variable\n",
    "features_shortlist.append('SalePrice')\n",
    "for i in range(0, len(features_shortlist), 5):\n",
    "    sns.pairplot(data=df4[features_shortlist], \n",
    "                 x_vars=df4[features_shortlist][:-1].columns[i:i+5], \n",
    "                 y_vars = ['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correlation heatmap of features shortlist\n",
    "df_xcorrs = df4[features_shortlist].corr()\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "sns.heatmap(df_xcorrs[(df_xcorrs >= 0.90) | (df_xcorrs <= -0.5)],\n",
    "            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1, annot=True, annot_kws={'size': 8}, square=True);\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(*sorted(features_shortlist), sep=', \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# features after eda and data manipulation\n",
    "selection_old =['1stFlrSF','AllFlrsSF','AllSF','BsmtQual','ExterGrade','ExterQual','FireplaceQu',\n",
    "                             'Foundation_PConc', 'FullBath','GarageCars','GarageScore','GarageYrBlt','KitchenQual',\n",
    "                             'MasVnrArea','OverallGrade','OverallQual','TotalBath','TotalBsmtSF','TotRmsAbvGrd',\n",
    "                             'YearBuilt','YearRemodAdd']\n",
    "\n",
    "\n",
    "selection = ['1stFlrSF', 'AllFlrsSF', 'AllSF', 'BsmtQual', 'ExterGrade', 'ExterQual', 'FireplaceQu', \n",
    "'Foundation_PConc', 'FullBath', 'GarageArea','GarageCars', 'KitchenQual', 'OverallGrade', 'OverallQual', \n",
    "'TotRmsAbvGrd', 'TotalBath', 'YearBuilt', 'YearRemodAdd', 'lat', 'lon']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# analyze selected features\n",
    "#fig, ax = plt.subplots(round(len(features_shortlist) / 3), 3, figsize=(12,12))\n",
    "\n",
    "#for i, ax in enumerate(fig.axes):\n",
    "#    if i < len(features_shortlist) - 1:\n",
    "#        sns.regplot(x=features_shortlist[i], y='SalePrice', data=df4[features_shortlist], ax=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
